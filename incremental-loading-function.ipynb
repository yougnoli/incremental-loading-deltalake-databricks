{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d25986-ddd5-478f-9c25-9bc9eab57886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a653d6-964f-40d3-bf86-7bc0ff4c3d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## The Original Incremental Load Function from DP-700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00b3018-7cfe-416e-ad26-068697815e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def incremental_load(spark, df_source: DataFrame, target_table: str, candidate_key: list):\n",
    "    try:\n",
    "        delta_table = DeltaTable.forName(spark, target_table)\n",
    "    except Exception:\n",
    "        try:\n",
    "            df_source.write.format('delta').mode('overwrite').saveAsTable(target_table)\n",
    "            print(f\"Table '{target_table}' created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Initial load for table {target_table} failed with error: {str(e)}\")\n",
    "            raise\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        change_detection_columns = [col for col in df_source.columns if col not in candidate_key]\n",
    "\n",
    "        match_condition = ' AND '.join([f'target.{col} = source.{col}' for col in candidate_key])\n",
    "        update_condition = ' OR '.join([f'target.{col} != source.{col}' for col in change_detection_columns])\n",
    "\n",
    "        update_expr = {col: f'source.{col}' for col in df_source.columns}\n",
    "\n",
    "        merge_operation = delta_table.alias('target').merge(\n",
    "            source=df_source.alias('source'),\n",
    "            condition=match_condition\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=update_condition,\n",
    "            set=update_expr\n",
    "        ).whenNotMatchedInsertAll()\n",
    "\n",
    "        merge_operation.execute()\n",
    "        print(f\"Incremental load completed successfully for table '{target_table}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Incremental load failed for table {target_table} with error: {str(e)}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19dbdef1-33d0-4ce8-a36d-40beab199ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b939885-fbb3-481b-ba34-023135bbeb20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_initial = [\n",
    "    (1, \"Mario Rossi\", \"2025-01-01\", 100),\n",
    "    (2, \"Luigi Bianchi\", \"2025-01-02\", 150),\n",
    "    (3, \"Carla Verdi\", \"2025-01-03\", 200)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"date\", \"amount\"]\n",
    "df_initial = spark.createDataFrame(data_initial, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4858c11c-b9d8-4bf0-af5a-cbca4662d01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Performing the Initial Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5209039-8d66-48d2-8259-fd7b84864adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'default.my_incremental_table' created successfully.\n"
     ]
    }
   ],
   "source": [
    "incremental_load(\n",
    "    spark,\n",
    "    df_source=df_initial,\n",
    "    target_table=\"default.my_incremental_table\",\n",
    "    candidate_key=[\"id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec01dcb-f8fb-45c3-8652-46adddad671e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+\n| id|         name|      date|amount|\n+---+-------------+----------+------+\n|  2|Luigi Bianchi|2025-01-02|   150|\n|  1|  Mario Rossi|2025-01-01|   100|\n|  3|  Carla Verdi|2025-01-03|   200|\n+---+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verify the results\n",
    "\n",
    "spark.sql(\"SELECT * FROM default.my_incremental_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdcf09b2-56f4-42fd-8275-bc07f0e850aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading New and Updated Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09af37c9-73bd-42ec-b8c9-544b2bf16cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental load completed successfully for table 'default.my_incremental_table'.\n"
     ]
    }
   ],
   "source": [
    "data_update = [\n",
    "    (1, \"Mario Rossi\", \"2025-01-01\", 120),  # Updated\n",
    "    (2, \"Luigi Bianchi\", \"2025-01-02\", 150), # Unchanged\n",
    "    (4, \"Anna Blu\", \"2025-01-04\", 300)        # New Record\n",
    "]\n",
    "df_update = spark.createDataFrame(data_update, columns)\n",
    "\n",
    "incremental_load(\n",
    "    spark,\n",
    "    df_source=df_update,\n",
    "    target_table=\"default.my_incremental_table\",\n",
    "    candidate_key=[\"id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f945a5-813c-4138-9020-4d5837c1f40e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+\n| id|         name|      date|amount|\n+---+-------------+----------+------+\n|  1|  Mario Rossi|2025-01-01|   120|\n|  2|Luigi Bianchi|2025-01-02|   150|\n|  3|  Carla Verdi|2025-01-03|   200|\n|  4|     Anna Blu|2025-01-04|   300|\n+---+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verify the updated table\n",
    "\n",
    "spark.sql(\"SELECT * FROM default.my_incremental_table ORDER BY id\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "incremental-loading-function",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}